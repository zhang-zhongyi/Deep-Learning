{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/zhongyizhang/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from seaborn import color_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BATCH_NORM_DECAY = 0.9\n",
    "_BATCH_NORM_EPSILON = 1e-05\n",
    "_LEAKY_RELU = 0.1\n",
    "_ANCHORS = [(10, 13), (16, 30), (33, 23),\n",
    "            (30, 61), (62, 45), (59, 119),\n",
    "            (116, 90), (156, 198), (373, 326)]\n",
    "_MODEL_SIZE = (416, 416) # refers to the input size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a batch normalization using a standard set of parameters\n",
    "def batch_norm(inputs, training, data_format):\n",
    "    return tf.layers.batch_normalization(\n",
    "        inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n",
    "        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON,\n",
    "        scale=True, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet implementation of fixed padding\n",
    "def fixed_padding(inputs, kernel_size, data_format):\n",
    "    pad_total = kernel_size - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n",
    "                                        [pad_beg, pad_end],\n",
    "                                        [pad_beg, pad_end]])\n",
    "    else:\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n",
    "                                        [pad_beg, pad_end], [0, 0]])\n",
    "    return padded_inputs\n",
    "\n",
    "# Strided 2-D convolution with explicit padding\n",
    "def conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1):\n",
    "    if strides > 1:\n",
    "        inputs = fixed_padding(inputs, kernel_size, data_format)\n",
    "\n",
    "    return tf.layers.conv2d(\n",
    "        inputs=inputs, filters=filters, kernel_size=kernel_size,\n",
    "        strides=strides, padding=('SAME' if strides == 1 else 'VALID'),\n",
    "        use_bias=False, data_format=data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a residual block for Darknet\n",
    "def darknet53_residual_block(inputs, filters, training, data_format, strides=1):\n",
    "    shortcut = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, strides=strides, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding( inputs, filters=2 * filters, kernel_size=3, strides=strides, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs += shortcut\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Creates Darknet53 model for feature extraction\n",
    "def darknet53(inputs, training, data_format):\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3, strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = darknet53_residual_block(inputs, filters=32, training=training, data_format=data_format)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3, strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(2):\n",
    "        inputs = darknet53_residual_block(inputs, filters=64, training=training, data_format=data_format)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3, strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(8):\n",
    "        inputs = darknet53_residual_block(inputs, filters=128, training=training, data_format=data_format)\n",
    "\n",
    "    route1 = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3, strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(8):\n",
    "        inputs = darknet53_residual_block(inputs, filters=256, training=training, data_format=data_format)\n",
    "\n",
    "    route2 = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3, strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(4):\n",
    "        inputs = darknet53_residual_block(inputs, filters=512, training=training, data_format=data_format)\n",
    "\n",
    "    return route1, route2, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates convolution operations layer used after Darknet\n",
    "def yolo_convolution_block(inputs, filters, training, data_format):\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    route = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    return route, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Yolo final detection layer\n",
    "def yolo_layer(inputs, n_classes, anchors, img_size, data_format):\n",
    "    n_anchors = len(anchors)\n",
    "\n",
    "    inputs = tf.layers.conv2d(inputs, filters=n_anchors * (5 + n_classes),\n",
    "                              kernel_size=1, strides=1, use_bias=True,\n",
    "                              data_format=data_format)\n",
    "\n",
    "    shape = inputs.get_shape().as_list()\n",
    "    grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3]\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
    "    inputs = tf.reshape(inputs, [-1, n_anchors * grid_shape[0] * grid_shape[1], 5 + n_classes])\n",
    "\n",
    "    strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1])\n",
    "\n",
    "    box_centers, box_shapes, confidence, classes = \\\n",
    "        tf.split(inputs, [2, 2, 1, n_classes], axis=-1)\n",
    "\n",
    "    x = tf.range(grid_shape[0], dtype=tf.float32)\n",
    "    y = tf.range(grid_shape[1], dtype=tf.float32)\n",
    "    x_offset, y_offset = tf.meshgrid(x, y)\n",
    "    x_offset = tf.reshape(x_offset, (-1, 1))\n",
    "    y_offset = tf.reshape(y_offset, (-1, 1))\n",
    "    x_y_offset = tf.concat([x_offset, y_offset], axis=-1)\n",
    "    x_y_offset = tf.tile(x_y_offset, [1, n_anchors])\n",
    "    x_y_offset = tf.reshape(x_y_offset, [1, -1, 2])\n",
    "    box_centers = tf.nn.sigmoid(box_centers)\n",
    "    box_centers = (box_centers + x_y_offset) * strides\n",
    "\n",
    "    anchors = tf.tile(anchors, [grid_shape[0] * grid_shape[1], 1])\n",
    "    box_shapes = tf.exp(box_shapes) * tf.to_float(anchors)\n",
    "\n",
    "    confidence = tf.nn.sigmoid(confidence)\n",
    "\n",
    "    classes = tf.nn.sigmoid(classes)\n",
    "\n",
    "    inputs = tf.concat([box_centers, box_shapes, confidence, classes], axis=-1)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsamples to `out_shape` using nearest neighbor interpolation\n",
    "def upsample(inputs, out_shape, data_format):\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
    "        new_height = out_shape[3]\n",
    "        new_width = out_shape[2]\n",
    "    else:\n",
    "        new_height = out_shape[2]\n",
    "        new_width = out_shape[1]\n",
    "\n",
    "    inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width))\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs non-max suppression separately for each class\n",
    "def non_max_suppression(inputs, n_classes, max_output_size, iou_threshold, confidence_threshold):\n",
    "    batch = tf.unstack(inputs)\n",
    "    boxes_dicts = []\n",
    "    for boxes in batch:\n",
    "        boxes = tf.boolean_mask(boxes, boxes[:, 4] > confidence_threshold)\n",
    "        classes = tf.argmax(boxes[:, 5:], axis=-1)\n",
    "        classes = tf.expand_dims(tf.to_float(classes), axis=-1)\n",
    "        boxes = tf.concat([boxes[:, :5], classes], axis=-1)\n",
    "\n",
    "        boxes_dict = dict()\n",
    "        for cls in range(n_classes):\n",
    "            mask = tf.equal(boxes[:, 5], cls)\n",
    "            mask_shape = mask.get_shape()\n",
    "            if mask_shape.ndims != 0:\n",
    "                class_boxes = tf.boolean_mask(boxes, mask)\n",
    "                boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes,\n",
    "                                                              [4, 1, -1],\n",
    "                                                              axis=-1)\n",
    "                boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1])\n",
    "                indices = tf.image.non_max_suppression(boxes_coords,\n",
    "                                                       boxes_conf_scores,\n",
    "                                                       max_output_size,\n",
    "                                                       iou_threshold)\n",
    "                class_boxes = tf.gather(class_boxes, indices)\n",
    "                boxes_dict[cls] = class_boxes[:, :5]\n",
    "\n",
    "        boxes_dicts.append(boxes_dict)\n",
    "\n",
    "    return boxes_dicts\n",
    "\n",
    "\n",
    "# Computes top left and bottom right points of the boxes\n",
    "def build_boxes(inputs):\n",
    "    center_x, center_y, width, height, confidence, classes = \\\n",
    "        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n",
    "\n",
    "    top_left_x = center_x - width / 2\n",
    "    top_left_y = center_y - height / 2\n",
    "    bottom_right_x = center_x + width / 2\n",
    "    bottom_right_y = center_y + height / 2\n",
    "\n",
    "    boxes = tf.concat([top_left_x, top_left_y,\n",
    "                       bottom_right_x, bottom_right_y,\n",
    "                       confidence, classes], axis=-1)\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolo v3 model class\n",
    "class Yolo_v3:\n",
    "    def __init__(self, n_classes, model_size, max_output_size, iou_threshold,\n",
    "                 confidence_threshold, data_format=None):\n",
    "        if not data_format:\n",
    "            if tf.test.is_built_with_cuda():\n",
    "                data_format = 'channels_first'\n",
    "            else:\n",
    "                data_format = 'channels_last'\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.model_size = model_size\n",
    "        self.max_output_size = max_output_size\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.data_format = data_format\n",
    "\n",
    "    # Add operations to detect boxes for a batch of input images\n",
    "    def __call__(self, inputs, training):\n",
    "        with tf.variable_scope('yolo_v3_model'):\n",
    "            if self.data_format == 'channels_first':\n",
    "                inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "            inputs = inputs / 255\n",
    "\n",
    "            route1, route2, inputs = darknet53(inputs, training=training,\n",
    "                                               data_format=self.data_format)\n",
    "\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=512, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect1 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[6:9],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1,\n",
    "                                          data_format=self.data_format)\n",
    "            inputs = batch_norm(inputs, training=training,\n",
    "                                data_format=self.data_format)\n",
    "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "            upsample_size = route2.get_shape().as_list()\n",
    "            inputs = upsample(inputs, out_shape=upsample_size,\n",
    "                              data_format=self.data_format)\n",
    "            axis = 1 if self.data_format == 'channels_first' else 3\n",
    "            inputs = tf.concat([inputs, route2], axis=axis)\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=256, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect2 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[3:6],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1,\n",
    "                                          data_format=self.data_format)\n",
    "            inputs = batch_norm(inputs, training=training,\n",
    "                                data_format=self.data_format)\n",
    "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "            upsample_size = route1.get_shape().as_list()\n",
    "            inputs = upsample(inputs, out_shape=upsample_size,\n",
    "                              data_format=self.data_format)\n",
    "            inputs = tf.concat([inputs, route1], axis=axis)\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=128, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect3 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[0:3],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = tf.concat([detect1, detect2, detect3], axis=1)\n",
    "\n",
    "            inputs = build_boxes(inputs)\n",
    "\n",
    "            boxes_dicts = non_max_suppression(\n",
    "                inputs, n_classes=self.n_classes,\n",
    "                max_output_size=self.max_output_size,\n",
    "                iou_threshold=self.iou_threshold,\n",
    "                confidence_threshold=self.confidence_threshold)\n",
    "\n",
    "            return boxes_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads images in a 4D array\n",
    "def load_images(img_names, model_size):\n",
    "    imgs = []\n",
    "\n",
    "    img = cv2.imread(img_names)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, model_size)\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = np.expand_dims(img[:, :, :3], axis=0)\n",
    "    imgs.append(img)\n",
    "\n",
    "    imgs = np.concatenate(imgs)\n",
    "\n",
    "    return imgs\n",
    "\n",
    "# Returns a list of class names read from `file_name`\n",
    "def load_class_names(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        class_names = f.read().splitlines()\n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draws detected boxes\n",
    "def draw_boxes(img_names, boxes_dicts, class_names, model_size):\n",
    "    colors = ((np.array(color_palette(\"hls\", 80)) * 255)).astype(np.uint8)\n",
    "    for num, img_name, boxes_dict in zip(range(len(img_names)), img_names, boxes_dicts):\n",
    "        \n",
    "        img = cv2.imread(img_names)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        resize_factor = (img.shape[1] / model_size[0], img.shape[0] / model_size[1])\n",
    "        for cls in range(len(class_names)):\n",
    "            boxes = boxes_dict[cls]\n",
    "            color = colors[cls]\n",
    "            color = tuple([int(x) for x in color])\n",
    "            if np.size(boxes) != 0:\n",
    "                for box in boxes:\n",
    "                    xy, confidence = box[:4], box[4]\n",
    "                    confidence = ' '+str(confidence*100)[:2]\n",
    "                    xy = [int(xy[i] * resize_factor[i % 2]) for i in range(4)]\n",
    "                    cv2.rectangle(img, (xy[0], xy[1]), (xy[2], xy[3]), color[::-1], 2)\n",
    "                    (test_width, text_height), baseline = cv2.getTextSize(class_names[cls]+confidence,\n",
    "                                                                          cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                                                          0.75, 1)\n",
    "                    cv2.rectangle(img, (xy[0], xy[1]),\n",
    "                                  (xy[0] + test_width, xy[1] - text_height - baseline),\n",
    "                                  color[::-1], thickness=cv2.FILLED)\n",
    "                    cv2.putText(img, class_names[cls]+confidence, (xy[0], xy[1] - baseline),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 0), 1)\n",
    "\n",
    "            if not os.path.exists('detections'):\n",
    "                os.mkdir('detections')\n",
    "                \n",
    "            head, tail = os.path.split(img_names)\n",
    "            name = './detections/'+tail[:-4]+'_yolo.jpg'\n",
    "            cv2.imwrite(name, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draws detected boxes in a video frame\n",
    "def draw_frame(frame, frame_size, boxes_dicts, class_names, model_size):\n",
    "    boxes_dict = boxes_dicts[0]\n",
    "    resize_factor = (frame_size[0] / model_size[1], frame_size[1] / model_size[0])\n",
    "    colors = ((np.array(color_palette(\"hls\", 80)) * 255)).astype(np.uint8)\n",
    "    for cls in range(len(class_names)):\n",
    "        boxes = boxes_dict[cls]\n",
    "        color = colors[cls]\n",
    "        color = tuple([int(x) for x in color])\n",
    "        if np.size(boxes) != 0:\n",
    "            for box in boxes:\n",
    "                xy, confidence = box[:4], box[4]\n",
    "                confidence = ''\n",
    "                #confidence = ' '+str(confidence*100)[:2]\n",
    "                xy = [int(xy[i] * resize_factor[i % 2]) for i in range(4)]\n",
    "                cv2.rectangle(frame, (xy[0], xy[1]), (xy[2], xy[3]), color[::-1], 2)\n",
    "                (test_width, text_height), baseline = cv2.getTextSize(class_names[cls]+confidence,\n",
    "                                                                      cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                                                      0.75, 1)\n",
    "                cv2.rectangle(frame, (xy[0], xy[1]),\n",
    "                              (xy[0] + test_width, xy[1] - text_height - baseline),\n",
    "                              color[::-1], thickness=cv2.FILLED)\n",
    "                cv2.putText(frame, class_names[cls]+confidence, (xy[0], xy[1] - baseline),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshapes and loads official pretrained Yolo weights.\n",
    "def load_weights(variables, file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        # Skip first 5 values containing irrelevant info\n",
    "        np.fromfile(f, dtype=np.int32, count=5)\n",
    "        weights = np.fromfile(f, dtype=np.float32)\n",
    "\n",
    "        assign_ops = []\n",
    "        ptr = 0\n",
    "\n",
    "        # Load weights for Darknet part.\n",
    "        # Each convolution layer has batch normalization.\n",
    "        for i in range(52):\n",
    "            conv_var = variables[5 * i]\n",
    "            gamma, beta, mean, variance = variables[5 * i + 1:5 * i + 5]\n",
    "            batch_norm_vars = [beta, gamma, mean, variance]\n",
    "\n",
    "            for var in batch_norm_vars:\n",
    "                shape = var.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(var, var_weights))\n",
    "\n",
    "            shape = conv_var.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                (shape[3], shape[2], shape[0], shape[1]))\n",
    "            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "        # Loading weights for Yolo part.\n",
    "        # 7th, 15th and 23rd convolution layer has biases and no batch norm.\n",
    "        ranges = [range(0, 6), range(6, 13), range(13, 20)]\n",
    "        unnormalized = [6, 13, 20]\n",
    "        for j in range(3):\n",
    "            for i in ranges[j]:\n",
    "                current = 52 * 5 + 5 * i + j * 2\n",
    "                conv_var = variables[current]\n",
    "                gamma, beta, mean, variance =  \\\n",
    "                    variables[current + 1:current + 5]\n",
    "                batch_norm_vars = [beta, gamma, mean, variance]\n",
    "\n",
    "                for var in batch_norm_vars:\n",
    "                    shape = var.shape.as_list()\n",
    "                    num_params = np.prod(shape)\n",
    "                    var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "                    ptr += num_params\n",
    "                    assign_ops.append(tf.assign(var, var_weights))\n",
    "\n",
    "                shape = conv_var.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                    (shape[3], shape[2], shape[0], shape[1]))\n",
    "                var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "            bias = variables[52 * 5 + unnormalized[j] * 5 + j * 2 + 1]\n",
    "            shape = bias.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(bias, var_weights))\n",
    "\n",
    "            conv_var = variables[52 * 5 + unnormalized[j] * 5 + j * 2]\n",
    "            shape = conv_var.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                (shape[3], shape[2], shape[0], shape[1]))\n",
    "            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "    return assign_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolo v3 image detection\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "from yolo_v3 import Yolo_v3\n",
    "from utils2 import load_images, load_class_names, draw_boxes\n",
    "\n",
    "_MODEL_SIZE = (416, 416)\n",
    "_CLASS_NAMES_FILE = 'coco.names'\n",
    "_MAX_OUTPUT_SIZE = 50\n",
    "\n",
    "detection_result = {}\n",
    "\n",
    "\n",
    "def main(iou_threshold, confidence_threshold, input_names):\n",
    "    global detection_result\n",
    "    class_names = load_class_names(_CLASS_NAMES_FILE)\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    model = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE,\n",
    "                    max_output_size=_MAX_OUTPUT_SIZE,\n",
    "                    iou_threshold=iou_threshold,\n",
    "                    confidence_threshold=confidence_threshold)\n",
    "\n",
    "\n",
    "    batch = load_images(input_names, model_size=_MODEL_SIZE)\n",
    "    inputs = tf.placeholder(tf.float32, [1, *_MODEL_SIZE, 3])\n",
    "    detections = model(inputs, training=False)\n",
    "    saver = tf.train.Saver(tf.global_variables(scope='yolo_v3_model'))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, '/Users/zhongyizhang/Desktop/YOL3/weights/model.ckpt')\n",
    "        detection_result = sess.run(detections, feed_dict={inputs: batch})\n",
    "\n",
    "    draw_boxes(input_names, detection_result, class_names, _MODEL_SIZE)\n",
    "\n",
    "    print('Detections have been saved successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable yolo_v3_model/conv2d/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/yolo_v3.py\", line 44, in conv2d_fixed_padding\n    use_bias=False, data_format=data_format)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/yolo_v3.py\", line 66, in darknet53\n    inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3, data_format=data_format)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/yolo_v3.py\", line 276, in __call__\n    data_format=self.data_format)\n  File \"<ipython-input-15-5d988b0d715e>\", line 31, in main\n    detections = model(inputs, training=False)\n  File \"<ipython-input-16-f4ea0b5e7b86>\", line 2, in <module>\n    main(0.5, 0.5, \"images/office.jpg\")\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f4ea0b5e7b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"images/office.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-f573e5d539c0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(iou_threshold, confidence_threshold, input_names)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_MODEL_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_MODEL_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'yolo_v3_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/yolo_v3.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             route1, route2, inputs = darknet53(inputs, training=training,\n\u001b[0;32m--> 276\u001b[0;31m                                                data_format=self.data_format)\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             route, inputs = yolo_convolution_block(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/yolo_v3.py\u001b[0m in \u001b[0;36mdarknet53\u001b[0;34m(inputs, training, data_format)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Creates Darknet53 model for feature extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdarknet53\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_fixed_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_LEAKY_RELU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/yolo_v3.py\u001b[0m in \u001b[0;36mconv2d_fixed_padding\u001b[0;34m(inputs, filters, kernel_size, data_format, strides)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SAME'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'VALID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         use_bias=False, data_format=data_format)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    422\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m       _scope=name)\n\u001b[0;32m--> 424\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \"\"\"\n\u001b[0;32m-> 1479\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m           \u001b[0;31m# Wrapping `call` function in autograph to allow for dynamic control\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m       \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m     \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m     \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mgetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 864\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable yolo_v3_model/conv2d/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/yolo_v3.py\", line 44, in conv2d_fixed_padding\n    use_bias=False, data_format=data_format)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/yolo_v3.py\", line 66, in darknet53\n    inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3, data_format=data_format)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/yolo_v3.py\", line 276, in __call__\n    data_format=self.data_format)\n  File \"<ipython-input-15-5d988b0d715e>\", line 31, in main\n    detections = model(inputs, training=False)\n  File \"<ipython-input-16-f4ea0b5e7b86>\", line 2, in <module>\n    main(0.5, 0.5, \"images/office.jpg\")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(0.5, 0.5, \"images/office.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
