{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "from yolo_v3 import Yolo_v3\n",
    "from utils.utils import load_images, load_class_names, draw_boxes\n",
    "from utils.utils import load_images, load_class_names, draw_boxes, draw_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m46\u001b[0m\n\u001b[0;31m    cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "_MODEL_SIZE = (416, 416)\n",
    "_CLASS_NAMES_FILE = '/Users/zhongyizhang/Desktop/YOLO3/data/labels/coco.names'\n",
    "_MAX_OUTPUT_SIZE = 20\n",
    "\n",
    "\n",
    "def main(iou_threshold, confidence_threshold, img_names):\n",
    "    batch_size = len(img_names)\n",
    "    batch = load_images(img_names, model_size=_MODEL_SIZE)\n",
    "def main(type, iou_threshold, confidence_threshold, input_names):\n",
    "    class_names = load_class_names(_CLASS_NAMES_FILE)\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "def main(iou_threshold, confidence_threshold, img_names):\n",
    "    iou_threshold=iou_threshold,\n",
    "    confidence_threshold=confidence_threshold)\n",
    "\n",
    "    inputs = tf.placeholder(tf.float32,\n",
    "                            [batch_size, *_MODEL_SIZE, 3])\n",
    "    if type == 'images':\n",
    "        batch_size = len(input_names)\n",
    "        batch = load_images(input_names, model_size=_MODEL_SIZE)\n",
    "        inputs = tf.placeholder(tf.float32, [batch_size, *_MODEL_SIZE, 3])\n",
    "        detections = model(inputs, training=False)\n",
    "        saver = tf.train.Saver(tf.global_variables(scope='yolo_v3_model'))\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, '/Users/zhongyizhang/Desktop/YOLO3/weights/model.ckpt')\n",
    "            detection_result = sess.run(detections, feed_dict={inputs: batch})\n",
    "\n",
    "        draw_boxes(input_names, detection_result, class_names, _MODEL_SIZE)\n",
    "\n",
    "        print('Detections have been saved successfully.')\n",
    "\n",
    "    elif type == 'video':\n",
    "        inputs = tf.placeholder(tf.float32, [1, *_MODEL_SIZE, 3])\n",
    "        detections = model(inputs, training=False)\n",
    "        saver = tf.train.Saver(tf.global_variables(scope='yolo_v3_model'))\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, '/Users/zhongyizhang/Desktop/YOLO3/weights/model.ckpt')\n",
    "\n",
    "            win_name = 'Video detection'\n",
    "            cv2.namedWindow(win_name)\n",
    "            cap = cv2.VideoCapture(input_names[0])\n",
    "            frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH),\n",
    "                          cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'X264')\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            out = cv2.VideoWriter('/Users/zhongyizhang/Desktop/YOLO3/detections/detections.mp4', fourcc, fps,\n",
    "                                  (int(frame_size[0]), int(frame_size[1])))\n",
    "\n",
    "            try:\n",
    "                while True:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    resized_frame = cv2.resize(frame, dsize=_MODEL_SIZE,\n",
    "                                               interpolation=cv2.INTER_NEAREST)\n",
    "                    detection_result = sess.run(detections,\n",
    "                                                feed_dict={inputs: [resized_frame]})\n",
    "\n",
    "                    draw_frame(frame, frame_size, detection_result,\n",
    "                               class_names, _MODEL_SIZE)\n",
    "\n",
    "                    cv2.imshow(win_name, frame)\n",
    "\n",
    "    detections = model(inputs, training=False)\n",
    "                    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables(scope='yolo_v3_model'))\n",
    "                    # Exit\n",
    "                    if key == ord('q'):\n",
    "                        break\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, '/Users/zhongyizhang/Desktop/YOLO3/weights/model.ckpt')\n",
    "        detection_result = sess.run(detections, feed_dict={inputs: batch})\n",
    "                    # Take screenshot\n",
    "                    if key == ord('s'):\n",
    "                        pass\n",
    "\n",
    "    draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE)\n",
    "                    out.write(frame)\n",
    "            finally:\n",
    "                cv2.destroyAllWindows()\n",
    "                cap.release()\n",
    "\n",
    "    print('Detections have been saved successfully.')\n",
    "    else:\n",
    "        raise ValueError(\"Inappropriate data type. Please choose either 'video' or 'images'.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(float(sys.argv[1]), float(sys.argv[2]), sys.argv[3:])\n",
    "    main(sys.argv[1], float(sys.argv[2]), float(sys.argv[3]), sys.argv[4:])\n",
    "    requirements.txt \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from seaborn import color_palette\n",
    "import cv2\n",
    "\n",
    "\n",
    "def load_images(img_names, model_size):\n",
    "        \"\"\"Loads images in a 4D array.\n",
    "        Args:\n",
    "            img_names: A list of images names.\n",
    "            model_size: The input size of the model.\n",
    "            data_format: A format for the array returned\n",
    "                ('channels_first' or 'channels_last').\n",
    "        Returns:\n",
    "            A 4D NumPy array.\n",
    "        \"\"\"\n",
    "def load_class_names(file_name):\n",
    "\n",
    "    def draw_boxes(img_names, boxes_dicts, class_names, model_size):\n",
    "        \"\"\"Draws detected boxes.\n",
    "        Args:\n",
    "            img_names: A list of input images names.\n",
    "            boxes_dict: A class-to-boxes dictionary.\n",
    "            class_names: A class names list.\n",
    "            model_size: The input size of the model.\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        colors = ((np.array(color_palette(\"hls\", 80)) * 255)).astype(np.uint8)\n",
    "        for num, img_name, boxes_dict in zip(range(len(img_names)), img_names,\n",
    "                                         boxes_dicts):\n",
    "        img = Image.open(img_name)\n",
    "def draw_boxes(img_names, boxes_dicts, class_names, model_size):\n",
    "        for cls in range(len(class_names)):\n",
    "            boxes = boxes_dict[cls]\n",
    "            if np.size(boxes) != 0:\n",
    "                color = np.random.permutation([np.random.randint(256), 255, 0])\n",
    "                color = colors[cls]\n",
    "                for box in boxes:\n",
    "                    xy, confidence = box[:4], box[4]\n",
    "                    xy = [xy[i] * resize_factor[i % 2] for i in range(4)]\n",
    "def draw_boxes(img_names, boxes_dicts, class_names, model_size):\n",
    "        rgb_img = img.convert('RGB')\n",
    "\n",
    "        rgb_img.save('/Users/zhongyizhang/Desktop/YOLO3/detections/detection_' + str(num + 1) + '.jpg')\n",
    "\n",
    "\n",
    "def draw_frame(frame, frame_size, boxes_dicts, class_names, model_size):\n",
    "    \"\"\"Draws detected boxes in a video frame.\n",
    "    Args:\n",
    "        frame: A video frame.\n",
    "        frame_size: A tuple of (frame width, frame height).\n",
    "        boxes_dicts: A class-to-boxes dictionary.\n",
    "        class_names: A class names list.\n",
    "        model_size:The input size of the model.\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    boxes_dict = boxes_dicts[0]\n",
    "    resize_factor = (frame_size[0] / model_size[1], frame_size[1] / model_size[0])\n",
    "    colors = ((np.array(color_palette(\"hls\", 80)) * 255)).astype(np.uint8)\n",
    "    for cls in range(len(class_names)):\n",
    "        boxes = boxes_dict[cls]\n",
    "        color = colors[cls]\n",
    "        color = tuple([int(x) for x in color])\n",
    "        if np.size(boxes) != 0:\n",
    "            for box in boxes:\n",
    "                xy = box[:4]\n",
    "                xy = [int(xy[i] * resize_factor[i % 2]) for i in range(4)]\n",
    "                cv2.rectangle(frame, (xy[0], xy[1]), (xy[2], xy[3]), color[::-1], 2)\n",
    "                (test_width, text_height), baseline = cv2.getTextSize(class_names[cls],\n",
    "                                                                      cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                                                      0.75, 1)\n",
    "                cv2.rectangle(frame, (xy[0], xy[1]),\n",
    "                              (xy[0] + test_width, xy[1] - text_height - baseline),\n",
    "                              color[::-1], thickness=cv2.FILLED)\n",
    "                cv2.putText(frame, class_names[cls], (xy[0], xy[1] - baseline),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
